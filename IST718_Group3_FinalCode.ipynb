{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Road Accident Traffic Prediction\n",
        "Eamon Gallagher, Eric Greenstein, Shubham Kumar\n",
        "<br><br>\n",
        "\n",
        "Even minor accidents can have an effect on traffic patterns. The goal of this project is to predict how long an accident will affect traffic based on the information available when it occurs.\n",
        "\n",
        "Knowing when an accident will clear is integral to many kinds of services. Web mapping services like Apple’s “Maps,” for example, rely on this information for quality routing.\n",
        "<br><br>\n",
        "\n",
        "Data Set: “US Accidents (Updated)” \n",
        "\n",
        "From kaggle.com:\n",
        "https://www.kaggle.com/sobhanmoosavi/us-accidents\n",
        "\n",
        "Includes data from 1.5 million US motor vehicle accidents from 2016-2020. data was collected from traffic APIs that compile a variety of information from a multitude of sources. \n",
        "\n",
        "The questions this project seeks to answer are :\n",
        "1. What factors affect an accident time to clear?\n",
        "2. Can we effectively predict how long in minutes an accident will affect traffic flow?\n",
        "<br>\n",
        "\n",
        "Every step of this project was completed using the pyspark module for apache spark. More information about spark can be found [here.](https://spark.apache.org/) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_nwiaBDSG-4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading necessary software, modules and functions into Google Colab:"
      ],
      "metadata": {
        "id": "m_suF5XiJKkp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBKeBd3j52wO"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myyKjDE63I-x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBR4gAFXpf6C"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "import numpy as np\n",
        "\n",
        "spark = SparkSession.builder.config(\"spark.executor.memory\",\"16g\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "import pyspark\n",
        "from pyspark.ml import feature, regression, Pipeline\n",
        "from pyspark.sql import functions as fn, Row\n",
        "from pyspark import sql\n",
        "from pyspark.sql.functions import expr, upper\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLLr1CsPCM4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3288c24-a45e-46c9-9960-16eeae416eca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# connecting to gdrive\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the Data"
      ],
      "metadata": {
        "id": "O7BCwBbauhfI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssogXt4kGpwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59744010-f317-48e2-ba8f-976f62c28b8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1516064"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# loading accidents as a spark dataframe\n",
        "# the path will need to be updated to wherever the data file is stored within your gdrive or system.\n",
        "accidents = spark.read.csv(path = 'gdrive/My Drive/Big Data Final Project/US_Accidents_Dec20_updated.csv', inferSchema=True, sep=',', header=True)\n",
        "\n",
        "# long format can be used to calculate the distance between times\n",
        "# length seconds represents the distance between the start and end times\n",
        "# timestamps can be used to retrieve hour of day, day of week, etc\n",
        "accidents = accidents.withColumn(\"Start_Timestamp\", fn.col(\"Start_Time\").cast(\"timestamp\").cast(\"long\"))\n",
        "accidents = accidents.withColumn(\"End_Timestamp\", fn.col(\"End_Time\").cast(\"timestamp\").cast(\"long\"))\n",
        "accidents = accidents.withColumn(\"Length_Seconds\",fn.col(\"End_Timestamp\") - fn.col(\"Start_Timestamp\"))\n",
        "accidents = accidents.withColumn(\"Start_Real_Timestamp\",fn.col(\"Start_Time\").cast(\"timestamp\"))\n",
        "accidents = accidents.withColumn(\"End_Real_Timestamp\",fn.col(\"End_Time\").cast(\"timestamp\"))\n",
        "\n",
        "# selecting columns needed from analysis\n",
        "# seconds in minutes and seconds and ours also added\n",
        "accidents2 = accidents.select(fn.col('Severity').cast('int'), \n",
        "                              (fn.col('Side') == \"R\").alias('Right_Side').cast('int'), \n",
        "                              fn.col('Start_Timestamp'), \n",
        "                              fn.col('End_Timestamp'), \n",
        "                              fn.col(\"Length_Seconds\"), \n",
        "                              fn.col('Start_Real_Timestamp'),\n",
        "                              fn.col('End_Real_Timestamp'),\n",
        "                              (fn.round(fn.col(\"Length_Seconds\")/60)).cast('int').alias(\"Length_Minutes\"), \n",
        "                              (fn.round(fn.col(\"Length_Seconds\")/3600)).cast('int').alias(\"Length_Hours\"),\n",
        "                              fn.col('Distance(mi)').alias('Distance_mi'), \n",
        "                              (fn.col('Number').isNotNull()).alias('Has_Number').cast('int'), \n",
        "                              fn.col('Timezone'), \n",
        "                              fn.col('Temperature(F)').cast(\"int\"), \n",
        "                              fn.col('Wind_Chill(F)').cast(\"int\"), \n",
        "                              fn.col('Humidity(%)').cast(\"int\"), \n",
        "                              fn.col('Pressure(in)').cast(\"int\"), \n",
        "                              fn.col('Weather_Condition'),\n",
        "                              (fn.col('Sunrise_Sunset') == 'Day').alias('Is_Day').cast('int'), \n",
        "                              fn.col('Description'),\n",
        "                              fn.col('Amenity').cast('int'),\n",
        "                              fn.col('Bump').cast('int'),\n",
        "                              fn.col('Crossing').cast('int'),\n",
        "                              fn.col('Give_Way').cast('int'),\n",
        "                              fn.col('Junction').cast('int'),\n",
        "                              fn.col('No_Exit').cast('int'),\n",
        "                              fn.col('Railway').cast('int'),\n",
        "                              fn.col('Roundabout').cast('int'), \n",
        "                              fn.col('Station').cast('int'), \n",
        "                              fn.col('Stop').cast('int'), \n",
        "                              fn.col('Traffic_Calming').cast('int'), \n",
        "                              fn.col('Traffic_Signal').cast('int'),\n",
        "                              fn.col('Turning_Loop').cast('int'),\n",
        "                              upper(fn.col('Wind_Direction')),\n",
        "                              fn.col('Visibility(mi)').cast(\"int\"),\n",
        "                              fn.col('Wind_Speed(mph)').cast(\"int\"),\n",
        "                              fn.col('Precipitation(in)').cast(\"int\"),\n",
        "                              fn.col('Start_Lat'),\n",
        "                              fn.col('Start_Lng'))\n",
        "\n",
        "accidents2.cache()\n",
        "accidents2.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRcQzcVxS3Yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb32227e-e476-4593-cc1e-d729cf25c99a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+---------------+-------------+--------------+--------------------+-------------------+--------------+------------+-----------+----------+----------+--------------+-------------+-----------+------------+-----------------+------+--------------------+-------+----+--------+--------+--------+-------+-------+----------+-------+----+---------------+--------------+------------+---------------------+--------------+---------------+-----------------+---------+---------+\n",
            "|Severity|Right_Side|Start_Timestamp|End_Timestamp|Length_Seconds|Start_Real_Timestamp| End_Real_Timestamp|Length_Minutes|Length_Hours|Distance_mi|Has_Number|  Timezone|Temperature(F)|Wind_Chill(F)|Humidity(%)|Pressure(in)|Weather_Condition|Is_Day|         Description|Amenity|Bump|Crossing|Give_Way|Junction|No_Exit|Railway|Roundabout|Station|Stop|Traffic_Calming|Traffic_Signal|Turning_Loop|upper(Wind_Direction)|Visibility(mi)|Wind_Speed(mph)|Precipitation(in)|Start_Lat|Start_Lng|\n",
            "+--------+----------+---------------+-------------+--------------+--------------------+-------------------+--------------+------------+-----------+----------+----------+--------------+-------------+-----------+------------+-----------------+------+--------------------+-------+----+--------+--------+--------+-------+-------+----------+-------+----+---------------+--------------+------------+---------------------+--------------+---------------+-----------------+---------+---------+\n",
            "|       3|         1|     1454891828|   1454913428|         21600| 2016-02-08 00:37:08|2016-02-08 06:37:08|           360|           6|       3.23|         0|US/Eastern|            42|           36|         58|          29|       Light Rain|     0|Between Sawmill R...|      0|   0|       0|       0|       0|      0|      0|         0|      0|   0|              0|             0|           0|                   SW|            10|             10|                0| 40.10891|-83.09286|\n",
            "|       2|         1|     1454910980|   1454932580|         21600| 2016-02-08 05:56:20|2016-02-08 11:56:20|           360|           6|      0.747|         0|US/Eastern|            36|         null|         91|          29|       Light Rain|     0|At OH-4/OH-235/Ex...|      0|   0|       0|       0|       0|      0|      0|         0|      0|   0|              0|             0|           0|                 CALM|            10|           null|                0| 39.86542| -84.0628|\n",
            "|       2|         1|     1454912139|   1454933739|         21600| 2016-02-08 06:15:39|2016-02-08 12:15:39|           360|           6|      0.055|         0|US/Eastern|            36|         null|         97|          29|         Overcast|     0|At I-71/US-50/Exi...|      0|   0|       0|       0|       1|      0|      0|         0|      0|   0|              0|             0|           0|                 CALM|            10|           null|                0| 39.10266|-84.52468|\n",
            "|       2|         1|     1454912139|   1454933739|         21600| 2016-02-08 06:15:39|2016-02-08 12:15:39|           360|           6|      0.219|         0|US/Eastern|            36|         null|         97|          29|         Overcast|     0|At I-71/US-50/Exi...|      0|   0|       0|       0|       1|      0|      0|         0|      0|   0|              0|             0|           0|                 CALM|            10|           null|                0| 39.10148|-84.52341|\n",
            "|       2|         1|     1454914305|   1454935905|         21600| 2016-02-08 06:51:45|2016-02-08 12:51:45|           360|           6|      0.123|         0|US/Eastern|            39|         null|         55|          29|         Overcast|     0|At Dart Ave/Exit ...|      0|   0|       0|       0|       0|      0|      0|         0|      0|   0|              0|             0|           0|                 CALM|            10|           null|             null| 41.06213|-81.53784|\n",
            "+--------+----------+---------------+-------------+--------------+--------------------+-------------------+--------------+------------+-----------+----------+----------+--------------+-------------+-----------+------------+-----------------+------+--------------------+-------+----+--------+--------+--------+-------+-------+----------+-------+----+---------------+--------------+------------+---------------------+--------------+---------------+-----------------+---------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "accidents2.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning\n",
        "\n",
        "The data was collected from several different sources, meaning there were NA values, duplicates, and other anomalies that need to be addressed before further analysis can be done\n",
        "\n",
        "### NAS, Duplicates and \"Six_Hour\" Observations"
      ],
      "metadata": {
        "id": "j7nFGYPtu-77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6ge5HLU_g65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81fd563c-191f-4d24-c7d1-b10102fb5ebb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f6509923810>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# This plot represents a large spike in number of accidents that lasted about 6 hours\n",
        "accidents2.select(fn.col('Length_Hours')).where(fn.col('Length_Hours') <= 20).toPandas().hist(column = \"Length_Hours\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHbKg15O3zKF"
      },
      "outputs": [],
      "source": [
        "# looking closer we realized many accidents lasted exactly 6 hours.\n",
        "# this is likely a default value for accidents not properly recorded\n",
        "# adding a column for observations with exactly 6 hours for length\n",
        "accidents2 = accidents2.withColumn(\"Six_Hours\", (fn.col('Length_Seconds') == 21600))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z529qkEs5HrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d783d56-1bad-43c4-a58f-f1cc438a2693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+------------------+-------------------+------------------+------------------+------------------+\n",
            "|Six_Hours|  Count|  avg(Distance_mi)|avg(Temperature(F))|avg(Wind_Chill(F))|  avg(Humidity(%))| avg(Pressure(in))|\n",
            "+---------+-------+------------------+-------------------+------------------+------------------+------------------+\n",
            "|     true| 367648|0.6993420445643391|  62.71783771443844| 27.54027156814042|63.455947357566096|29.534066336713366|\n",
            "|    false|1148416|0.5513809011717078|  58.45236421487166| 56.72459204378561| 65.04805726431225|28.898813527377836|\n",
            "+---------+-------+------------------+-------------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# most columns don't show a significant difference for numerical columns when length is exactly 6 hours\n",
        "# wind chill is much lower, meaning it was probably recorded differently or had a default value of 0\n",
        "# an average windchill 30 degrees lower than average temperature is more than just unlikely\n",
        "accidents2.groupby('Six_Hours').agg(fn.count('Has_Number').alias('Count'), fn.avg('Distance_mi'), fn.avg('Temperature(F)'), fn.avg('Wind_Chill(F)'), \n",
        "                                    fn.avg('Humidity(%)'), fn.avg('Pressure(in)')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz_1CrkQ-vv-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9fbf8e1-313f-4668-8022-e08ee7ab7a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+------------------+-------------------+------------------+-----------------+------------------+\n",
            "|Six_Hours| Count|  avg(Distance_mi)|avg(Temperature(F))|avg(Wind_Chill(F))| avg(Humidity(%))| avg(Pressure(in))|\n",
            "+---------+------+------------------+-------------------+------------------+-----------------+------------------+\n",
            "|     true| 15278|1.0542181568268074|  39.61886372561854|32.896845136798014| 79.8393114281974|  29.2154732294803|\n",
            "|    false|926601|0.5322343112083914| 58.888931697677855| 57.64104398764948|65.09654209309078|28.812603267209944|\n",
            "+---------+------+------------------+-------------------+------------------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# after dropping nas, we can see about 85% of the Six_Hours observations had NAs in relevant columns\n",
        "# this is compared to only about 10% for the rest of the observations\n",
        "# temperature is much lower for this subset of 6 hour observations.\n",
        "# altogether we can conclude that these observations probably came from a different source\n",
        "accidents2 = accidents2.na.drop()\n",
        "accidents2.groupby('Six_Hours').agg( fn.count('Has_Number').alias('Count'), fn.avg('Distance_mi'), fn.avg('Temperature(F)'), fn.avg('Wind_Chill(F)'), fn.avg('Humidity(%)'), fn.avg('Pressure(in)')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaCKgEaOFwKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a880ae0e-4728-465f-a491-be6b13c8f30b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------+------------------+-------------------+------------------+-----------------+------------------+\n",
            "|Six_Hours| Count|  avg(Distance_mi)|avg(Temperature(F))|avg(Wind_Chill(F))| avg(Humidity(%))| avg(Pressure(in))|\n",
            "+---------+------+------------------+-------------------+------------------+-----------------+------------------+\n",
            "|     true| 15182|1.0603868396785667|  39.58009484916348|32.846923988934265|79.85081016993809|29.215518377025425|\n",
            "|    false|816572|0.5112853232782903|  59.31383025624195|  58.0811869620805|64.67303311894113|28.787361065527595|\n",
            "+---------+------+------------------+-------------------+------------------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# for good measure, lets look at the distinct observations left\n",
        "# out of the remaining observations, the Six_Hour observations had a much smaller percentage of duplicates\n",
        "accidents2 = accidents2.distinct()\n",
        "accidents2.groupby('Six_Hours').agg(fn.count('Has_Number').alias('Count'), fn.avg('Distance_mi'), fn.avg('Temperature(F)'), fn.avg('Wind_Chill(F)'), fn.avg('Humidity(%)'), fn.avg('Pressure(in)')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK3IsYSN2_Q1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df29616-96b2-409e-ca79-b4ea4ff4c825"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "816572"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# after duplicates, observations with NAs in relevant columns and Six_Hour observations,\n",
        "# we are left with 816,572 observations\n",
        "accidents3 = accidents2.where(fn.col('Six_Hours') == False)\n",
        "accidents2.unpersist()\n",
        "accidents3.cache()\n",
        "accidents3.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTPD--lZATAc"
      },
      "source": [
        "### Removing Outliers\n",
        "\n",
        "Outliers can skew a model, making it less effective at prediction of standard observations. Given the continuous nature of the problem and our regression based solutions, we have decided to remove outliers to best predict the result of more common cases.\n",
        "\n",
        "### Outliers in length of traffic\n",
        "There were some extreme outliers (thousands of hours) which absolutely wrecked the predictive capability of the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiPeM95nATAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9314b89-30e9-4539-81e4-c01d48631a2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outliers for Length of accident are greater than 282.0 minutes, or 4.7 hours\n"
          ]
        }
      ],
      "source": [
        "# outliers were determined using Q1 - IQR and Q3 + IQR\n",
        "# almost all accidents lasted under 5 hours\n",
        "quartiles = accidents3.agg(fn.expr(\"percentile(Length_Minutes, array(.25, .5, .75))\"))\n",
        "firstQuartile = quartiles.first()[0][0]\n",
        "median = quartiles.first()[0][1]\n",
        "thirdQuartile = quartiles.first()[0][2]\n",
        "\n",
        "iqr = thirdQuartile - firstQuartile\n",
        "Q3_plus_iqr = thirdQuartile + iqr \n",
        "\n",
        "print('Outliers for Length of accident are greater than', Q3_plus_iqr, 'minutes, or', Q3_plus_iqr/60, 'hours')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIN1aHTkATAd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b49058-6421-4349-c70c-a1ea13089424"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "752281"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# 752,281 observations recorded accidents with length under 5 hours\n",
        "# these are the observations we will use to build our model\n",
        "accidents3 = accidents3.where(fn.col('Length_Hours')<=5)\n",
        "accidents3.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional Outliers: Pressure(in) and Distance(mi)\n"
      ],
      "metadata": {
        "id": "jo7os8GcyUh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We decided IQR is a good representation of outliers in other columns"
      ],
      "metadata": {
        "id": "eMb0-6S3vc_A"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0XU7cZKc20I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f62abec2-376a-44c5-8fd6-4ae2916f5e2f"
      },
      "source": [
        "accidents3 = accidents3.where(fn.col('Pressure(in)')<=29)\n",
        "accidents3 = accidents3.where(fn.col('Distance_mi')<=2.5)\n",
        "accidents3.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "584760"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGSJX06lKRqS"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time"
      ],
      "metadata": {
        "id": "lQOCdCqSkWRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Hour Bins"
      ],
      "metadata": {
        "id": "QF-5X7bHkYiz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVkaXLEbKUIl"
      },
      "outputs": [],
      "source": [
        "# This Function allows us to pull the hours from the timesatmps present in the data\n",
        "from pyspark.sql.functions import hour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X-KXIiAKUOD"
      },
      "outputs": [],
      "source": [
        "# Adding a column for hour of the day (Numerical)\n",
        "accidents3 = accidents3.withColumn('Hour_Of_The_Day',hour(accidents2.Start_Real_Timestamp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUG_j3rBKUZW"
      },
      "outputs": [],
      "source": [
        "# This function allows us to easily create categorical groups for numerical variables.\n",
        "from pyspark.ml.feature import Bucketizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-YdKsOTKUc_"
      },
      "outputs": [],
      "source": [
        "# The first function selects the splits for the groupings, every group is split by \"Lower Range\" <= Xi < \"Upper Range\" \n",
        "bucketizer = Bucketizer(splits=[ 0, 2, 4, 6,8,10,12,14,16,18,20,22, float('Inf') ],inputCol=\"Hour_Of_The_Day\", outputCol=\"Hour_Buckets\")\n",
        "# This creates a new column in our data set that has each of the buckets for time\n",
        "accidents3 = bucketizer.setHandleInvalid(\"keep\").transform(accidents3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQNR8Y8aKUgz"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNUbeAATKgPZ"
      },
      "outputs": [],
      "source": [
        "# This first line creates a dictionary for the buckets\n",
        "# The second line creates a udf to itterate through the data easily\n",
        "# The third line adds the categorical buckets column to the data set\n",
        "Buckets = {0.0:\"12am to 2am\", 1.0: \"2am to 4am\", 2.0:\"4am to 6am\", 3.0: \"6am to 8am\", 4.0: \"8am to 10am\", 5.0: \"10am to 12pm\", 6.0: \"12pm to 2pm\", 7.0: \"2pm to 4pm\", 8.0: \"4pm to 6pm\", 9.0: \"6pm to 8pm\", 10.0: \"8pm to 10pm\", 11.0: \"10pm to 12am\"}\n",
        "udf = udf(lambda x: Buckets[x], StringType())\n",
        "accidents3 = accidents3.withColumn(\"Hour_Buckets_Named\", udf(\"Hour_Buckets\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7UzCvalKgSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b1093b-de72-4ce0-c999-7ff16d0cd0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+---------------+\n",
            "|Hour_Buckets_Named|Hour_Of_The_Day|\n",
            "+------------------+---------------+\n",
            "|        2pm to 4pm|             15|\n",
            "|       12am to 2am|              1|\n",
            "|       8am to 10am|              8|\n",
            "|        6am to 8am|              7|\n",
            "|        2am to 4am|              2|\n",
            "|       12am to 2am|              0|\n",
            "|      10pm to 12am|             22|\n",
            "|       12am to 2am|              1|\n",
            "|       12am to 2am|              1|\n",
            "|        2am to 4am|              3|\n",
            "+------------------+---------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Checking results of buckting\n",
        "# These categorical values will be indexed during the model building process\n",
        "accidents3.select(fn.col(\"Hour_Buckets_Named\"),fn.col(\"Hour_Of_The_Day\")).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eZiOo8jnNO2"
      },
      "source": [
        "### Adding Day of the week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3mUw-o2nPbW"
      },
      "outputs": [],
      "source": [
        "# This function allows us to pull the days of the week from the time stamps in categorical form\n",
        "from pyspark.sql.functions import date_format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IMIZyZ-nPkh"
      },
      "outputs": [],
      "source": [
        "# Adding the days of the week column to the data\n",
        "accidents3 = accidents3.withColumn(\"day_of_the_week\", date_format(fn.col(\"Start_Real_Timestamp\"), \"EEEE\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOv0UkJunUlk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd264c8-7f27-4bc3-dea2-807774a75fbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+\n",
            "|day_of_the_week|\n",
            "+---------------+\n",
            "|       Thursday|\n",
            "|        Tuesday|\n",
            "|         Friday|\n",
            "|      Wednesday|\n",
            "|       Thursday|\n",
            "|       Thursday|\n",
            "|       Saturday|\n",
            "|         Friday|\n",
            "|       Saturday|\n",
            "|      Wednesday|\n",
            "+---------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Checking results\n",
        "# Once again these categorical values can be indexed for our final model\n",
        "accidents3.select(\"day_of_the_week\").show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weather\n"
      ],
      "metadata": {
        "id": "WGKarYpP0iXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Bins for Temperature"
      ],
      "metadata": {
        "id": "NkE58luRkgRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The first function selects the splits for the groupings, every group is split by \"Lower Range\" <= Xi < \"Upper Range\" \n",
        "TempBucketizer = Bucketizer(splits=[ -50, 10, 20, 30,40,50,60,70,80,90,100, float('Inf') ],inputCol=\"Temperature(F)\", outputCol=\"Temp_Buckets\")\n",
        "# This creates a new column in our data set that has each of the buckets for temperature\n",
        "accidents3 = TempBucketizer.setHandleInvalid(\"keep\").transform(accidents3)"
      ],
      "metadata": {
        "id": "dIprIGJA0pgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This first line creates a dictionary for the buckets\n",
        "# The second line creates a udf to itterate through the data easily\n",
        "# The third line adds the categorical buckets column to the data set\n",
        "\n",
        "''''\n",
        "Temp_Buckets = {0.0:\"Less Than 10 degrees\", 1.0: \"10 to 20 degrees\", 2.0:\"20 to 30 degrees\", 3.0: \"30 to 40 degrees\", 4.0: \"40 to 50 degrees\", 5.0: \"50 to 60 degrees\", 6.0: \"60 to 70 degrees\", 7.0: \"70 to 80 degrees\", 8.0: \"80 to 90 degrees\", 9.0: \"90 to 100 degrees\", 10.0: \"Over 100 degrees\"}\n",
        "Temp_udf = udf(lambda x: Temp_Buckets[x], StringType())\n",
        "accidents3 = accidents3.withColumn(\"Temp_Buckets_Named\", Temp_udf(\"Temp_Buckets\"))\n",
        "'''\n",
        "\n",
        "\"This portion of code was origninally developed in Jupyter Notebook. It throws an unknown error in Google Colab and so these buckets cannot be used for the final model\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "TaRjvQF-0uX-",
        "outputId": "65450a42-c2a7-4fc0-d892-db861d1fbcb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This portion of code was origninally developed in Jupyter Notebook. It throws an unknown error in Google Colab and so these buckets cannot be used for the final model'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weather Conditions\n",
        "\n",
        "The dataframe includes a brief description of the weather at the time of the crash. The condition can include one or multiple kinds of weather, some of which are pretty rare. Different weather conditions have different effect on traffic time. "
      ],
      "metadata": {
        "id": "UMk6DG_T2QmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accidents3.groupby('Weather_Condition').agg(fn.avg('Length_Minutes'), fn.count('Weather_Condition').alias('Count')).sort('Count', ascending=False).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctELgJtw1Rnt",
        "outputId": "e83bafea-27df-4bdc-dc3d-a43901f658b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+------+\n",
            "|   Weather_Condition|avg(Length_Minutes)| Count|\n",
            "+--------------------+-------------------+------+\n",
            "|                Fair|  93.17526120405515|264257|\n",
            "|              Cloudy|  96.37553725286368|100046|\n",
            "|       Mostly Cloudy|  82.52021134202184| 70975|\n",
            "|       Partly Cloudy|   82.7685733198349| 51364|\n",
            "|          Light Rain|  92.99518506483916| 33853|\n",
            "|          Light Snow| 103.45998274705262| 10433|\n",
            "|                 Fog| 112.93167016226957|  9059|\n",
            "|                Rain|  83.20840862546682|  8301|\n",
            "|                Haze|  95.46460644479662|  7572|\n",
            "|        Fair / Windy|    86.409578270193|  4197|\n",
            "|          Heavy Rain|  76.94145420207744|  3177|\n",
            "|      Cloudy / Windy|  93.59349955476402|  2246|\n",
            "|Mostly Cloudy / W...|  74.49877511024008|  2041|\n",
            "|       Light Drizzle|  90.51203133743705|  1787|\n",
            "|               Smoke| 125.88995215311004|  1463|\n",
            "|Partly Cloudy / W...|  75.38368580060423|  1324|\n",
            "|  Light Rain / Windy| 100.35887096774194|  1240|\n",
            "|                Snow| 104.12811980033278|  1202|\n",
            "|             T-Storm|   65.3602199816682|  1091|\n",
            "|          Wintry Mix| 115.34814814814816|   945|\n",
            "+--------------------+-------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# there are almost 100 distinct weather conditions within the dataframe\n",
        "# because an observation can include 1 or more types of weather condition, \n",
        "# we decided on a sparse matrix that includes top types of weather condtions\n",
        "# fair, the most common weather condition, we be left out to serve as a baseline\n",
        "accidents3 = accidents3.withColumn('Cloud', (fn.col('Weather_Condition').contains('Cloud')).cast('int')).\\\n",
        "             withColumn('Partly', (fn.col('Weather_Condition').contains('Partly')).cast('int')).\\\n",
        "             withColumn('Mostly', (fn.col('Weather_Condition').contains('Mostly')).cast('int')).\\\n",
        "             withColumn('Rain', (fn.col('Weather_Condition').contains('Rain')).cast('int')).\\\n",
        "             withColumn('Fog', (fn.col('Weather_Condition').contains('Fog')).cast('int')).\\\n",
        "             withColumn('Snow', (fn.col('Weather_Condition').contains('Snow')).cast('int')).\\\n",
        "             withColumn('Clear', (fn.col('Weather_Condition').contains('Clear')).cast('int')).\\\n",
        "             withColumn('Haze', (fn.col('Weather_Condition').contains('Haze')).cast('int')).\\\n",
        "             withColumn('Wind', (fn.col('Weather_Condition').contains('Wind')).cast('int')).\\\n",
        "             withColumn('Smoke', (fn.col('Weather_Condition').contains('Smoke')).cast('int')).\\\n",
        "             withColumn('Light', (fn.col('Weather_Condition').contains('Light')).cast('int')).\\\n",
        "             withColumn('Heavy', (fn.col('Weather_Condition').contains('Heavy')).cast('int')).\\\n",
        "             withColumn('Overcast', (fn.col('Weather_Condition').contains('Overcast')).cast('int')).\\\n",
        "             withColumn('Thunder', (fn.col('Weather_Condition').contains('T-Storm') | fn.col('Weather_Condition').contains('Thunder')).cast('int'))\n",
        "\n",
        "weatherCols = ['Cloud', 'Partly', 'Mostly', 'Rain', 'Fog', 'Snow', 'Clear', 'Haze',\n",
        "               'Wind', 'Smoke', 'Light', 'Heavy', 'Overcast', 'Thunder']\n",
        "accidents3.select(weatherCols).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x6tw4yD3KrT",
        "outputId": "2ab05ca8-a4a6-4de2-eaa9-c4670ebb4fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+------+----+---+----+-----+----+----+-----+-----+-----+--------+-------+\n",
            "|Cloud|Partly|Mostly|Rain|Fog|Snow|Clear|Haze|Wind|Smoke|Light|Heavy|Overcast|Thunder|\n",
            "+-----+------+------+----+---+----+-----+----+----+-----+-----+-----+--------+-------+\n",
            "|    1|     0|     0|   0|  0|   0|    0|   0|   0|    0|    0|    0|       0|      0|\n",
            "|    0|     0|     0|   0|  0|   0|    0|   0|   0|    0|    0|    0|       0|      0|\n",
            "|    0|     0|     0|   0|  0|   0|    0|   0|   0|    0|    0|    0|       0|      0|\n",
            "|    0|     0|     0|   1|  0|   0|    0|   0|   0|    0|    1|    0|       0|      0|\n",
            "|    0|     0|     0|   0|  0|   0|    0|   0|   0|    0|    0|    0|       0|      0|\n",
            "|    0|     0|     0|   1|  0|   0|    0|   0|   0|    0|    1|    0|       0|      0|\n",
            "|    0|     0|     0|   0|  0|   0|    0|   0|   0|    0|    0|    0|       0|      0|\n",
            "|    1|     0|     0|   0|  0|   0|    0|   0|   0|    0|    0|    0|       0|      0|\n",
            "|    0|     0|     0|   0|  0|   0|    0|   0|   0|    0|    0|    0|       0|      0|\n",
            "|    0|     0|     0|   0|  0|   0|    0|   0|   0|    0|    0|    0|       0|      0|\n",
            "|    0|     0|     0|   0|  0|   0|    0|   0|   0|    1|    0|    0|       0|      0|\n",
            "|    0|     0|     0|   0|  0|   0|    0|   0|   0|    0|    0|    0|       0|      0|\n",
            "|    0|     0|     0|   0|  0|   0|    0|   0|   0|    0|    0|    0|       0|      0|\n",
            "|    1|     1|     0|   0|  0|   0|    0|   0|   0|    0|    0|    0|       0|      0|\n",
            "|    0|     0|     0|   0|  0|   0|    0|   0|   0|    0|    0|    0|       0|      0|\n",
            "|    0|     0|     0|   0|  0|   0|    0|   0|   0|    0|    0|    0|       0|      0|\n",
            "|    0|     0|     0|   0|  0|   0|    0|   0|   0|    0|    0|    0|       0|      1|\n",
            "|    0|     0|     0|   0|  0|   1|    0|   0|   0|    0|    1|    0|       0|      0|\n",
            "|    1|     0|     0|   0|  0|   0|    0|   0|   0|    0|    0|    0|       0|      0|\n",
            "|    0|     0|     0|   0|  0|   0|    0|   0|   0|    0|    0|    0|       0|      0|\n",
            "+-----+------+------+----+---+----+-----+----+----+-----+-----+-----+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LCNQeI99_2U"
      },
      "source": [
        "## Location\n",
        "<br>\n",
        "\n",
        "### Description Analysis\n",
        "Location features presented the biggest challenge in terms of incorporating them into the model. The dataset includes observations across the entire US. Clustering the data is difficult because each region, state and even city includes many different types of roads with different traffic pattern. The best column we had for quantifying location was a description column, a short account of where the accident occured."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qy0kGBWr4yop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50f1f741-c9e4-4cb6-f945-d4a9fafd927e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------------------+-------------+\n",
            "|       bigram|avg(Length_Minutes)|count(bigram)|\n",
            "+-------------+-------------------+-------------+\n",
            "|  incident on| 131.29520788111537|       137747|\n",
            "|   drive with| 130.75613952581412|        87181|\n",
            "| with caution| 130.75613952581412|        87181|\n",
            "|       due to|  99.81667907669397|        47005|\n",
            "|  to accident|  96.55219985085756|        45594|\n",
            "|  rd accident|  76.61827093865283|        58552|\n",
            "|exit accident|  67.45095555535009|       108157|\n",
            "| ave accident|  62.24963092202389|        37255|\n",
            "| lane blocked|  49.49755772168786|        46473|\n",
            "+-------------+-------------------+-------------+\n",
            "\n",
            "+-------------+-------------------+-------------+\n",
            "|       bigram|avg(Length_Minutes)|count(bigram)|\n",
            "+-------------+-------------------+-------------+\n",
            "| lane blocked|  49.49755772168786|        46473|\n",
            "| ave accident|  62.24963092202389|        37255|\n",
            "|exit accident|  67.45095555535009|       108157|\n",
            "|  rd accident|  76.61827093865283|        58552|\n",
            "|  to accident|  96.55219985085756|        45594|\n",
            "|       due to|  99.81667907669397|        47005|\n",
            "| with caution| 130.75613952581412|        87181|\n",
            "|   drive with| 130.75613952581412|        87181|\n",
            "|  incident on| 131.29520788111537|       137747|\n",
            "+-------------+-------------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# first we performed bigram analysis\n",
        "from pyspark.ml.feature import NGram\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "# regex tokenizer creates a list of words\n",
        "tokenizer = feature.RegexTokenizer(minTokenLength=2)\\\n",
        "  .setGaps(False)\\\n",
        "  .setPattern(\"\\\\p{L}+\")\\\n",
        "  .setInputCol(\"Description\")\\\n",
        "  .setOutputCol(\"words\")\n",
        "\n",
        "# ngram collects pairs of words into a list of bigrams\n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")\n",
        "\n",
        "# a new DF contains the words and the bigrams as a new column\n",
        "bigramDF = Pipeline(stages = [tokenizer, ngram]).fit(accidents3).transform(accidents3)\n",
        "\n",
        "# finally, we can observe which common bigrams are associated with long and short traffic\n",
        "topBigrams = bigramDF.select(explode(\"bigrams\").alias(\"bigram\"), fn.col(\"Length_Minutes\")).\\\n",
        "             groupBy(\"bigram\").agg(fn.avg(\"Length_Minutes\"), fn.count(\"bigram\")).where(fn.col(\"count(bigram)\")>=30000)\n",
        "\n",
        "topBigrams.orderBy(\"avg(Length_Minutes)\", ascending = False).show(10)\n",
        "topBigrams.orderBy(\"avg(Length_Minutes)\", ascending = True).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Topic Modeling"
      ],
      "metadata": {
        "id": "NsL3-9nH8HfE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln1_lRheLNNF"
      },
      "outputs": [],
      "source": [
        "# next we decided to use clustering to create a feature from our description\n",
        "from pyspark.ml import clustering\n",
        "\n",
        "# cv creates a token frequency for words in each description\n",
        "# minDF is set to 10,000 so only words appearing this many times appear in the tf\n",
        "# this is to ensure that the matrix doesn't become to sparse (there are 750,000 observations)\n",
        "cv = feature.CountVectorizer(inputCol='words', outputCol='tf', minDF = 10000)\n",
        "\n",
        "# idf boosts the significance of words that appear less frequently\n",
        "idf = feature.IDF(inputCol='tf', outputCol='tfidf')\n",
        "\n",
        "# finally lda, topic modeling, is used to creat the features\n",
        "lda = clustering.LDA(k=8, featuresCol='tfidf', topicDistributionCol='lda_feat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWePq5OHLNQE"
      },
      "outputs": [],
      "source": [
        "# this pipeline was used to add the features to a new lda_df\n",
        "lda_model = Pipeline(stages= [tokenizer, cv, idf, lda]).fit(accidents3)\n",
        "lda_df = lda_model.transform(accidents3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYkxuOnTxFON"
      },
      "outputs": [],
      "source": [
        "# from the lda portion of our model\n",
        "ldaStage = lda_model.stages[-1]\n",
        "# get the terms\n",
        "terms_matrix = np.array(ldaStage.describeTopics(4).rdd.map(lambda x: x['termIndices']).collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvNbpa2jyBJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb6ae47a-bc03-4ea8-80f4-85b19c27dcb5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['shoulder', 'nb', 'hard', 'blocked'],\n",
              "       ['dr', 'eb', 'incident', 'near'],\n",
              "       ['rd', 'blvd', 'at', 'exit'],\n",
              "       ['caution', 'drive', 'with', 'near'],\n",
              "       ['to', 'ave', 'from', 'traffic'],\n",
              "       ['closed', 'lane', 'blocked', 'road'],\n",
              "       ['delays', 'expect', 'cr', 'incident'],\n",
              "       ['st', 'th', 'ca', 'vehicle']], dtype='<U10')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# we can see the top words for each topic\n",
        "# an observation will be more closely correlated with a topic if it shares more top words\n",
        "vocabulary = lda_model.stages[1].vocabulary\n",
        "np.array(vocabulary)[terms_matrix]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bigram Columns\n",
        "<br>\n",
        "\n",
        "Adding the LDA features to the model did improve performance. Unfortunately, the features also more than tripled the time that the model took to train. Using a similar solution to weather features, another sparse matrix for description bigrams was created. It includes the most common bigrams with minimal overlap. At the end of the day, this too made the dataframe too large and extended training time past what would be reasonable."
      ],
      "metadata": {
        "id": "rSDzxs948GMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the new matrix to a new dataframe, accidents4\n",
        "accidents4 = bigramDF.withColumn(\"expect_delays\", fn.col(\"bigrams\").cast(\"string\").contains(\"expect delays\").cast(\"int\")).\\\n",
        "                      withColumn(\"incident_on\", fn.col(\"bigrams\").cast(\"string\").contains(\"incident on\").cast(\"int\")).\\\n",
        "                      withColumn(\"th_st\", fn.col(\"bigrams\").cast(\"string\").contains(\"th st\").cast(\"int\")).\\\n",
        "                      withColumn(\"due_to\", fn.col(\"bigrams\").cast(\"string\").contains(\"due to\").cast(\"int\")).\\\n",
        "                      withColumn(\"rd_exit\", fn.col(\"bigrams\").cast(\"string\").contains(\"rd exit\").cast(\"int\")).\\\n",
        "                      withColumn(\"road_closed\", fn.col(\"bigrams\").cast(\"string\").contains(\"road closed\").cast(\"int\")).\\\n",
        "                      withColumn(\"rd_accident\", fn.col(\"bigrams\").cast(\"string\").contains(\"rd accident\").cast(\"int\")).\\\n",
        "                      withColumn(\"st_accident\", fn.col(\"bigrams\").cast(\"string\").contains(\"st accident\").cast(\"int\")).\\\n",
        "                      withColumn(\"expect_delays\", fn.col(\"bigrams\").cast(\"string\").contains(\"expect delays\").cast(\"int\")).\\\n",
        "                      withColumn(\"exit_accident\", fn.col(\"bigrams\").cast(\"string\").contains(\"exit accident\").cast(\"int\")).\\\n",
        "                      withColumn(\"ave_accident\", fn.col(\"bigrams\").cast(\"string\").contains(\"ave accident\").cast(\"int\")).\\\n",
        "                      withColumn(\"lane_blocked\", fn.col(\"bigrams\").cast(\"string\").contains(\"lane blocked\").cast(\"int\"))\n",
        "\n",
        "bigramCols = [\"expect_delays\",\n",
        "              \"incident_on\",\n",
        "              \"th_st\",\n",
        "              \"due_to\",\n",
        "              \"rd_exit\",\n",
        "              \"road_closed\",\n",
        "              \"rd_accident\",\n",
        "              \"st_accident\",\n",
        "              \"expect_delays\",\n",
        "              \"exit_accident\",\n",
        "              \"ave_accident\",\n",
        "              \"lane_blocked\"]\n",
        "\n",
        "accidents4.select(bigramCols).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdb5RfFIjorY",
        "outputId": "c5ed6ec1-064d-41ba-db11-3688fd37bbe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----------+-----+------+-------+-----------+-----------+-----------+-------------+-------------+------------+------------+\n",
            "|expect_delays|incident_on|th_st|due_to|rd_exit|road_closed|rd_accident|st_accident|expect_delays|exit_accident|ave_accident|lane_blocked|\n",
            "+-------------+-----------+-----+------+-------+-----------+-----------+-----------+-------------+-------------+------------+------------+\n",
            "|            0|          1|    0|     0|      0|          0|          0|          0|            0|            0|           0|           0|\n",
            "|            0|          1|    0|     0|      0|          0|          0|          0|            0|            0|           0|           0|\n",
            "|            0|          1|    0|     0|      0|          0|          0|          0|            0|            0|           0|           0|\n",
            "|            0|          0|    0|     0|      0|          0|          0|          0|            0|            0|           0|           0|\n",
            "|            0|          0|    0|     1|      0|          0|          0|          0|            0|            0|           0|           0|\n",
            "|            0|          1|    0|     0|      0|          0|          0|          0|            0|            0|           0|           0|\n",
            "|            0|          1|    0|     0|      0|          0|          0|          0|            0|            0|           0|           0|\n",
            "|            0|          1|    0|     0|      0|          0|          0|          0|            0|            0|           0|           1|\n",
            "|            0|          1|    0|     0|      0|          0|          0|          0|            0|            0|           0|           1|\n",
            "|            0|          1|    0|     0|      0|          0|          0|          0|            0|            0|           0|           0|\n",
            "|            0|          1|    0|     0|      0|          0|          0|          0|            0|            0|           0|           0|\n",
            "|            0|          0|    0|     0|      0|          0|          0|          0|            0|            0|           0|           0|\n",
            "|            0|          1|    0|     0|      0|          0|          0|          0|            0|            0|           0|           0|\n",
            "|            0|          0|    1|     1|      0|          0|          0|          0|            0|            0|           0|           0|\n",
            "|            0|          0|    0|     1|      0|          0|          0|          0|            0|            0|           0|           0|\n",
            "|            0|          1|    0|     0|      0|          0|          0|          0|            0|            0|           0|           0|\n",
            "|            0|          0|    0|     1|      0|          0|          0|          0|            0|            0|           0|           0|\n",
            "|            1|          1|    0|     0|      0|          0|          0|          0|            1|            0|           0|           1|\n",
            "|            0|          0|    0|     0|      0|          0|          0|          0|            0|            0|           0|           0|\n",
            "|            1|          1|    0|     0|      0|          0|          0|          0|            1|            0|           0|           1|\n",
            "+-------------+-----------+-----+------+-------+-----------+-----------+-----------+-------------+-------------+------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Features: Binary and Continuous Columns\n",
        "\n",
        "These do not need to be adjusted before the model building step."
      ],
      "metadata": {
        "id": "wMwkCyCI4YMO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp3q-uZHbGxv"
      },
      "outputs": [],
      "source": [
        "# Points of Interest\n",
        "# These features are binary and can be added to the model as is\n",
        "categoricalFeatures = ['Amenity', \n",
        "               'Bump', \n",
        "               'Crossing', \n",
        "               'Give_Way',  \n",
        "               'Junction',\n",
        "               'No_Exit', \n",
        "               'Railway',\n",
        "               'Roundabout',\n",
        "               'Station',\n",
        "               'Stop',\n",
        "               'Traffic_Calming',\n",
        "               'Traffic_Signal',\n",
        "               'Turning_Loop'\n",
        "               ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPzpyS_zbR1c"
      },
      "outputs": [],
      "source": [
        "# these features will be scaled using a standard scaler\n",
        "scalingFeatures = ['Temperature(F)', \n",
        "                   'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', \n",
        "                   'Distance_mi', 'Visibility(mi)', 'Wind_Speed(mph)', \n",
        "                   'Precipitation(in)']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ],
      "metadata": {
        "id": "MotMWPFS_psV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting Data into train, validation, and test"
      ],
      "metadata": {
        "id": "Xv_6SVP0mhww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_df, validation_df, testing_df = accidents3.randomSplit([0.6, 0.3, 0.1], seed=0)"
      ],
      "metadata": {
        "id": "7FWetAjRmh_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1KdFVmqdt_i"
      },
      "source": [
        "### Model I: Linear regression with all POIs & scaling features(distance included), and top weather conditions\n",
        "\n",
        "We use StandardScaler to scale Numerical features, it means to subtract by the mean and scaling to unit variance.\n",
        "\n",
        "StringIndexer is used to convert string categoeries into nummerical categories; which then are passed through a OneHotEncoder to create Dummy variables.\n",
        "\n",
        "One Hot Encoding is a method used to create dummy variables from a column containing numerical groups of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyNpEDCibWgr"
      },
      "outputs": [],
      "source": [
        "Indexer = Pipeline(\n",
        "      stages=[\n",
        "            feature.VectorAssembler(inputCols=scalingFeatures, \n",
        "                                    outputCol='featuresEng'),\n",
        "            feature.StandardScaler(withMean=True, \n",
        "                                   inputCol='featuresEng', outputCol='featuresEngScaled'),\n",
        "            feature.StringIndexer(inputCol='upper(Wind_Direction)', outputCol=\"Wind_dir_StringFeatures\"),\n",
        "            feature.StringIndexer(inputCol='Timezone', outputCol=\"TimezoneStringFeatures\")\n",
        "            ])\n",
        "\n",
        "stringIndexedCols = ['Wind_dir_StringFeatures', 'TimezoneStringFeatures']\n",
        "\n",
        "encoders1 = [\n",
        "            feature.OneHotEncoder(\n",
        "                inputCol=col,\n",
        "                outputCol=\"{0}_OHEFeatures\".format(col))\n",
        "            for col in stringIndexedCols\n",
        "            ]\n",
        "\n",
        "assembler1 = feature.VectorAssembler(\n",
        "    inputCols=[encoder.getOutputCol() for encoder in encoders1], \n",
        "    outputCol=\"OHEFeatures1\")\n",
        "\n",
        "encoders2 = [\n",
        "            feature.OneHotEncoder(\n",
        "                inputCol=col,\n",
        "                outputCol=\"{0}_OHEFeatures\".format(col))\n",
        "            for col in categoricalFeatures\n",
        "            ]\n",
        "assembler2 = feature.VectorAssembler(\n",
        "    inputCols=[encoder.getOutputCol() for encoder in encoders2], \n",
        "    outputCol=\"OHEFeatures2\")\n",
        "\n",
        "weatherAssembler = feature.VectorAssembler(\n",
        "    inputCols= weatherCols,\n",
        "               outputCol=\"weatherFeatures\")\n",
        "\n",
        "finalAssembler = feature.VectorAssembler(\n",
        "    inputCols=[\"featuresEngScaled\", \"OHEFeatures1\", \"OHEFeatures2\", \"weatherFeatures\"],\n",
        "    outputCol=\"allFeatures\")\n",
        "\n",
        "reg = regression.LinearRegression(featuresCol='allFeatures', labelCol='Length_Minutes')\n",
        "\n",
        "pipe_model = Pipeline(stages=[Indexer] + encoders1 + [assembler1] + encoders2 + \n",
        "                      [assembler2] + [weatherAssembler] + [finalAssembler]+[reg])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use RMSE as the evaluation metric to measure our model's performance on the validation set."
      ],
      "metadata": {
        "id": "WxRXuayPpNel"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTPr610Hapj0",
        "outputId": "48b616be-9fba-40be-a98e-5eddf840cfd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE model I:  65.93470948561756\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import evaluation\n",
        "regression_evaluator = evaluation.RegressionEvaluator(labelCol='Length_Minutes', metricName='rmse')\n",
        "model1 = pipe_model.fit(training_df)\n",
        "print(\"RMSE model I: \", regression_evaluator.evaluate(model1.transform(validation_df)))\n",
        "print(\"R2 model I: \", regression_evaluator.evaluate(model1.transform(validation_df), {regression_evaluator.metricName: \"r2\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model makes an error of 65.9 mins(on average) in making predictions."
      ],
      "metadata": {
        "id": "TlmvDBrUrQkl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptQIta0si5Pn"
      },
      "source": [
        "### Model II: Reduced binary features with all weather conditions\n",
        "Many of the binary POI columns had false values thoughout.  Model II only includes the POIs with atleast a 10% presence of POIs. In addition, all weather conditions are included in this model instead of only using the weather condition matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvmtfpoojKa4"
      },
      "source": [
        "# The next model includes only a few categorical features, as many did not \n",
        "categorical_reduced_cols = [\n",
        "               'Crossing',\n",
        "               'Junction',\n",
        "               'Traffic_Signal',\n",
        "               'Right_Side',\n",
        "               'Has_Number',\n",
        "               'Is_Day'\n",
        "               ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Indexer = Pipeline(\n",
        "      stages=[\n",
        "            feature.VectorAssembler(inputCols=scalingFeatures, \n",
        "                                    outputCol='featuresEng'),\n",
        "            feature.StandardScaler(withMean=True, \n",
        "                                   inputCol='featuresEng', outputCol='featuresEngScaled'),\n",
        "            feature.StringIndexer(inputCol='upper(Wind_Direction)', outputCol=\"WindDirStringFeatures\"),\n",
        "            feature.StringIndexer(inputCol='Timezone', outputCol=\"TimezoneStringFeatures\"),\n",
        "            feature.StringIndexer(inputCol='Weather_Condition', outputCol=\"Weather_ConditionStringFeatures\", handleInvalid='keep')\n",
        "            ])\n",
        "\n",
        "stringIndexedCols = ['WindDirStringFeatures', 'TimezoneStringFeatures', \n",
        "                     'Weather_ConditionStringFeatures']\n",
        "encoders1 = [\n",
        "            feature.OneHotEncoder(\n",
        "                inputCol=col,\n",
        "                outputCol=\"{0}_OHEFeatures\".format(col))\n",
        "            for col in stringIndexedCols\n",
        "            ]\n",
        "\n",
        "assembler1 = feature.VectorAssembler(\n",
        "    inputCols=[encoder.getOutputCol() for encoder in encoders1], \n",
        "    outputCol=\"OHEFeatures1\")\n",
        "\n",
        "encoders2 = [\n",
        "            feature.OneHotEncoder(\n",
        "                inputCol=col,\n",
        "                outputCol=\"{0}_OHEFeatures\".format(col))\n",
        "            for col in categorical_reduced_cols\n",
        "            ]\n",
        "\n",
        "assembler2 = feature.VectorAssembler(\n",
        "    inputCols=[encoder.getOutputCol() for encoder in encoders], \n",
        "    outputCol=\"OHEFeatures2\")\n",
        "\n",
        "finalAssembler = feature.VectorAssembler(\n",
        "    inputCols=[\"featuresEngScaled\",\"OHEFeatures1\", \"OHEFeatures2\"],\n",
        "    outputCol=\"features\")\n",
        "\n",
        "reg = regression.LinearRegression(featuresCol='features', labelCol='Length_Minutes')\n",
        "\n",
        "pipe_model = Pipeline(stages=[Indexer] + encoders1 + [assembler1] + encoders2 + [assembler2] + [finalAssembler] + [reg])"
      ],
      "metadata": {
        "id": "a7nmxtWw-JMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = pipe_model.fit(training_df)\n",
        "print(\"RMSE model II: \", regression_evaluator.evaluate(model2.transform(validation_df)))\n",
        "print(\"R2 model II: \", regression_evaluator.evaluate(model2.transform(validation_df), {regression_evaluator.metricName: \"r2\"}))"
      ],
      "metadata": {
        "id": "rf_-0vE4-K8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af26447c-ea97-4c05-d088-a4068e286494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE model II:  63.852504793836744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model performs better than Model I. It makes an error of 63.8 mins on average."
      ],
      "metadata": {
        "id": "nKTeAWpPr2Vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model III: Including Hour of Day & Day of week\n",
        "Hour of day & Day of week are two string categorical column, which are StringIndexed and a OneHotEncoded an additional features"
      ],
      "metadata": {
        "id": "DtmYMAci-S6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Indexer = Pipeline(\n",
        "    stages=[\n",
        "            feature.VectorAssembler(inputCols=scalingFeatures, \n",
        "                                    outputCol='featuresEng'),\n",
        "            feature.StandardScaler(withMean=True, \n",
        "                                   inputCol='featuresEng', outputCol='featuresEngScaled'),\n",
        "            feature.StringIndexer(inputCol='upper(Wind_Direction)', outputCol=\"WindDirStringFeatures\"),\n",
        "            feature.StringIndexer(inputCol='Timezone', outputCol=\"TimezoneStringFeatures\"),\n",
        "            feature.StringIndexer(inputCol='Weather_Condition', outputCol=\"Weather_ConditionStringFeatures\", handleInvalid='keep'),\n",
        "            feature.StringIndexer(inputCol='Hour_Buckets_Named', outputCol=\"Hour_Buckets_Named_features\"),\n",
        "            feature.StringIndexer(inputCol='day_of_the_week', outputCol=\"day_of_the_week_features\")\n",
        "            ])\n",
        "stringIndexedCols = ['WindDirStringFeatures', 'TimezoneStringFeatures', \n",
        "                     'Weather_ConditionStringFeatures', 'Hour_Buckets_Named_features',\n",
        "                     'day_of_the_week_features']\n",
        "encoders1 = [\n",
        "            feature.OneHotEncoder(\n",
        "                inputCol=col,\n",
        "                outputCol=\"{0}_OHEFeatures\".format(col))\n",
        "            for col in stringIndexedCols\n",
        "            ]\n",
        "\n",
        "assembler1 = feature.VectorAssembler(\n",
        "    inputCols=[encoder.getOutputCol() for encoder in encoders1], \n",
        "    outputCol=\"OHEFeatures1\")\n",
        "\n",
        "encoders2 = [\n",
        "            feature.OneHotEncoder(\n",
        "                inputCol=col,\n",
        "                outputCol=\"{0}_OHEFeatures\".format(col))\n",
        "            for col in categorical_reduced_cols\n",
        "            ]\n",
        "assembler2 = feature.VectorAssembler(\n",
        "    inputCols=[encoder.getOutputCol() for encoder in encoders2], \n",
        "    outputCol=\"OHEFeatures2\")\n",
        "\n",
        "finalAssembler = feature.VectorAssembler(\n",
        "    inputCols=[\"featuresEngScaled\",\"OHEFeatures1\", \"OHEFeatures2\"],\n",
        "    outputCol=\"features\")\n",
        "\n",
        "reg = regression.LinearRegression(featuresCol='features', labelCol='Length_Minutes')\n",
        "\n",
        "pipe_model = Pipeline(stages=[Indexer] + encoders1 + [assembler1] + encoders2 + [assembler2] + [finalAssembler] + [reg])"
      ],
      "metadata": {
        "id": "nKM6F6gV-Vve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = pipe_model.fit(training_df)\n",
        "print(\"RMSE model III: \", regression_evaluator.evaluate(model3.transform(validation_df)))\n",
        "print(\"R2 model III: \", regression_evaluator.evaluate(model3.transform(validation_df), {regression_evaluator.metricName: \"r2\"}))"
      ],
      "metadata": {
        "id": "duI_w5Kz-XVk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f45a380e-cde6-4d36-b623-ea76bdc18d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE model III:  63.34772474072276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new features improve our model a little bit further!"
      ],
      "metadata": {
        "id": "Z1oraqMZwVFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model IV: Linear regression with L1 penalty\n",
        "We experiment next with adding regularization to our linear regression model. The function below is commented out because it takes time to run. However, the results of regularization are given below.\n",
        "\n",
        "The elasticNetParam in Linear regression supports L2 and L1regression based on coefficients as 0, 1 respectively."
      ],
      "metadata": {
        "id": "e-JkmVbB-ad6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment_with_regularization(param):\n",
        "  Indexer = Pipeline(\n",
        "      stages=[\n",
        "              feature.VectorAssembler(inputCols=scalingFeatures, \n",
        "                                      outputCol='featuresEng'),\n",
        "              feature.StandardScaler(withMean=True, \n",
        "                                    inputCol='featuresEng', outputCol='featuresEngScaled'),\n",
        "              feature.StringIndexer(inputCol='upper(Wind_Direction)', outputCol=\"WindDirStringFeatures\"),\n",
        "              feature.StringIndexer(inputCol='Timezone', outputCol=\"TimezoneStringFeatures\"),\n",
        "              feature.StringIndexer(inputCol='Weather_Condition', outputCol=\"Weather_ConditionStringFeatures\", handleInvalid='keep'),\n",
        "              feature.StringIndexer(inputCol='Hour_Buckets_Named', outputCol=\"Hour_Buckets_Named_features\"),\n",
        "              feature.StringIndexer(inputCol='day_of_the_week', outputCol=\"day_of_the_week_features\")\n",
        "              ])\n",
        "  stringIndexedCols = ['WindDirStringFeatures', 'TimezoneStringFeatures', \n",
        "                      'Weather_ConditionStringFeatures', 'Hour_Buckets_Named_features',\n",
        "                      'day_of_the_week_features']\n",
        "  encoders1 = [\n",
        "              feature.OneHotEncoder(\n",
        "                  inputCol=col,\n",
        "                  outputCol=\"{0}_OHEFeatures\".format(col))\n",
        "              for col in stringIndexedCols\n",
        "              ]\n",
        "\n",
        "  assembler1 = feature.VectorAssembler(\n",
        "      inputCols=[encoder.getOutputCol() for encoder in encoders1], \n",
        "      outputCol=\"OHEFeatures1\")\n",
        "\n",
        "  encoders2 = [\n",
        "              feature.OneHotEncoder(\n",
        "                  inputCol=col,\n",
        "                  outputCol=\"{0}_OHEFeatures\".format(col))\n",
        "              for col in categorical_reduced_cols\n",
        "              ]\n",
        "  assembler2 = feature.VectorAssembler(\n",
        "      inputCols=[encoder.getOutputCol() for encoder in encoders2], \n",
        "      outputCol=\"OHEFeatures2\")\n",
        "\n",
        "  finalAssembler = feature.VectorAssembler(\n",
        "      inputCols=[\"featuresEngScaled\",\"OHEFeatures1\", \"OHEFeatures2\"],\n",
        "      outputCol=\"features\")\n",
        "\n",
        "  reg = regression.LinearRegression(featuresCol='features', labelCol='Length_Minutes', elasticNetParam=param)\n",
        "\n",
        "  pipe_model = Pipeline(stages=[Indexer] + encoders1 + [assembler1] + encoders2 + [assembler2] + [finalAssembler] + [reg])\n",
        "  pred=pipe_model.fit(training_df).transform(validation_df)\n",
        "  print(\"RMSE: \", regression_evaluator.evaluate(pred))\n",
        "  print(\"R2: \", regression_evaluator.evaluate(pred, {regression_evaluator.metricName: \"r2\"}))"
      ],
      "metadata": {
        "id": "325379b2-YEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for elasticNetParam in [0, 1]:\n",
        "  if(elasticNetParam==0):\n",
        "    print(\"L2 Regularization\")\n",
        "  else:\n",
        "    print(\"L1 Regularization\")\n",
        "  experiment_with_regularization(elasticNetParam)\n",
        "  print()"
      ],
      "metadata": {
        "id": "2CkbJ4fV-g9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1f4935f-746d-4033-e18f-6168ce725601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 Regularization\n",
            "RMSE:  63.34772474072276\n",
            "\n",
            "L1 Regularization\n",
            "RMSE:  63.34768105552316\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results from our regularization experiment:\n",
        "\n",
        "L2 Regularization\n",
        "RMSE:  63.34772474072276\n",
        "\n",
        "L1 Regularization\n",
        "RMSE:  63.34768105552316\n",
        "\n",
        "We don't see much improvement in the model. Next we explore Random Forest Algorithm."
      ],
      "metadata": {
        "id": "UhXAMoHkzVKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model V: RandomForest Regressor"
      ],
      "metadata": {
        "id": "TBn2YubJ-qGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Indexer = Pipeline(\n",
        "    stages=[\n",
        "            feature.VectorAssembler(inputCols=scalingFeatures, \n",
        "                                    outputCol='featuresEng'),\n",
        "            feature.StandardScaler(withMean=True, \n",
        "                                   inputCol='featuresEng', outputCol='featuresEngScaled'),\n",
        "            feature.StringIndexer(inputCol='upper(Wind_Direction)', outputCol=\"WindDirStringFeatures\"),\n",
        "            feature.StringIndexer(inputCol='Timezone', outputCol=\"TimezoneStringFeatures\"),\n",
        "            feature.StringIndexer(inputCol='Weather_Condition', outputCol=\"Weather_ConditionStringFeatures\", handleInvalid='keep'),\n",
        "            feature.StringIndexer(inputCol='Hour_Buckets_Named', outputCol=\"Hour_Buckets_Named_features\"),\n",
        "            feature.StringIndexer(inputCol='day_of_the_week', outputCol=\"day_of_the_week_features\")\n",
        "            ])\n",
        "stringIndexedCols = ['WindDirStringFeatures', 'TimezoneStringFeatures', \n",
        "                     'Weather_ConditionStringFeatures', 'Hour_Buckets_Named_features',\n",
        "                     'day_of_the_week_features']\n",
        "encoders1 = [\n",
        "            feature.OneHotEncoder(\n",
        "                inputCol=col,\n",
        "                outputCol=\"{0}_OHEFeatures\".format(col))\n",
        "            for col in stringIndexedCols\n",
        "            ]\n",
        "\n",
        "assembler1 = feature.VectorAssembler(\n",
        "    inputCols=[encoder.getOutputCol() for encoder in encoders1], \n",
        "    outputCol=\"OHEFeatures1\")\n",
        "\n",
        "encoders2 = [\n",
        "            feature.OneHotEncoder(\n",
        "                inputCol=col,\n",
        "                outputCol=\"{0}_OHEFeatures\".format(col))\n",
        "            for col in categorical_reduced_cols\n",
        "            ]\n",
        "assembler2 = feature.VectorAssembler(\n",
        "    inputCols=[encoder.getOutputCol() for encoder in encoders2], \n",
        "    outputCol=\"OHEFeatures2\")\n",
        "\n",
        "finalAssembler = feature.VectorAssembler(\n",
        "    inputCols=[\"featuresEngScaled\",\"OHEFeatures1\", \"OHEFeatures2\"],\n",
        "    outputCol=\"features\")\n",
        "\n",
        "reg = regression.RandomForestRegressor(featuresCol='features', labelCol='Length_Minutes', maxDepth=10)\n",
        "\n",
        "pipe_model = Pipeline(stages=[Indexer] + encoders1 + [assembler1] + encoders2 + [assembler2] + [finalAssembler] + [reg])"
      ],
      "metadata": {
        "id": "Km3diFqPy8XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model5 = pipe_model.fit(training_df)\n",
        "print(\"RMSE model V: \", regression_evaluator.evaluate(model5.transform(validation_df)))\n",
        "print(\"R2 model V: \", regression_evaluator.evaluate(model5.transform(validation_df), {regression_evaluator.metricName: \"r2\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZDBySgON34R",
        "outputId": "52c6bc96-4fdb-4b2c-e16f-69475438eb22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE model VII:  58.096415565389954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest takes longer to run but produces a better RMSE than linear Regression"
      ],
      "metadata": {
        "id": "zxqJ0tlC3fC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model VI: RF Regression with PCA of reduced categorical cols\n",
        "\n",
        "We use Principal Component Analysis(PCA) here to create 2 Prinicipal compenents from all the POI variables. This will help to reduce our model training time and will allow us to keep the important features from all the POIs without getting rid of any columns.\n",
        "\n",
        "We also add Latitude and Longitude of the accident to our model as numerical features."
      ],
      "metadata": {
        "id": "se63re_v-wCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scalingFeatures.append(\"Start_Lat\")\n",
        "scalingFeatures.append(\"Start_Lng\")"
      ],
      "metadata": {
        "id": "50Sb3SJGJdkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Indexer = Pipeline(\n",
        "    stages=[\n",
        "            feature.VectorAssembler(inputCols=scalingFeatures, \n",
        "                                    outputCol='featuresEng'),\n",
        "            feature.StandardScaler(withMean=True, \n",
        "                                   inputCol='featuresEng', outputCol='featuresEngScaled'),\n",
        "            feature.StringIndexer(inputCol='upper(Wind_Direction)', outputCol=\"WindDirStringFeatures\"),\n",
        "            feature.StringIndexer(inputCol='Timezone', outputCol=\"TimezoneStringFeatures\"),\n",
        "            feature.StringIndexer(inputCol='Weather_Condition', outputCol=\"Weather_ConditionStringFeatures\", handleInvalid='keep'),\n",
        "            feature.StringIndexer(inputCol='Hour_Buckets_Named', outputCol=\"Hour_Buckets_Named_features\"),\n",
        "            feature.StringIndexer(inputCol='day_of_the_week', outputCol=\"day_of_the_week_features\")\n",
        "            ])\n",
        "stringIndexedCols = ['WindDirStringFeatures', 'TimezoneStringFeatures', \n",
        "                     'Weather_ConditionStringFeatures', 'Hour_Buckets_Named_features',\n",
        "                     'day_of_the_week_features']\n",
        "encoders1 = [\n",
        "            feature.OneHotEncoder(\n",
        "                inputCol=col,\n",
        "                outputCol=\"{0}_OHEFeatures\".format(col))\n",
        "            for col in stringIndexedCols\n",
        "            ]\n",
        "\n",
        "assembler1 = feature.VectorAssembler(\n",
        "    inputCols=[encoder.getOutputCol() for encoder in encoders1], \n",
        "    outputCol=\"OHEFeatures1\")\n",
        "\n",
        "encoders2 = [\n",
        "            feature.OneHotEncoder(\n",
        "                inputCol=col,\n",
        "                outputCol=\"{0}_OHEFeatures\".format(col))\n",
        "            for col in categoricalFeatures\n",
        "            ]\n",
        "\n",
        "assembler2 = feature.VectorAssembler(\n",
        "    inputCols=[encoder.getOutputCol() for encoder in encoders2], \n",
        "    outputCol=\"OHEFeatures2\")\n",
        "\n",
        "pca = feature.PCA(k=2, inputCol='OHEFeatures2', outputCol='pcaFeature')\n",
        "\n",
        "finalAssembler = feature.VectorAssembler(\n",
        "    inputCols=[\"featuresEngScaled\",\"OHEFeatures1\", \"pcaFeature\"],\n",
        "    outputCol=\"features\")\n",
        "\n",
        "reg = regression.RandomForestRegressor(featuresCol='features', labelCol='Length_Minutes', maxDepth=10)\n",
        "\n",
        "pipe_model = Pipeline(stages=[Indexer] + encoders1 + [assembler1] + encoders2 + [assembler2] + [pca] + [finalAssembler] + [reg])"
      ],
      "metadata": {
        "id": "vnuCIePg5JlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model6 = pipe_model.fit(training_df)\n",
        "print(\"RMSE model VI: \", regression_evaluator.evaluate(model6.transform(validation_df)))\n",
        "print(\"R2 model VI: \", regression_evaluator.evaluate(model6.transform(validation_df), {regression_evaluator.metricName: \"r2\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPAgcq9hOyQA",
        "outputId": "34597208-1e5d-4cce-8608-9026cfa5c635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE model IX:  54.29391903316418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We were able to reduce the RMSE to 54.3 mins!"
      ],
      "metadata": {
        "id": "VciAPaCmFLEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model VII: Decision Tree Regressor"
      ],
      "metadata": {
        "id": "u4heXndr_XYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Indexer = Pipeline(\n",
        "    stages=[\n",
        "            feature.VectorAssembler(inputCols=scalingFeatures, \n",
        "                                    outputCol='featuresEng'),\n",
        "            feature.StandardScaler(withMean=True, \n",
        "                                   inputCol='featuresEng', outputCol='featuresEngScaled'),\n",
        "            feature.StringIndexer(inputCol='upper(Wind_Direction)', outputCol=\"WindDirStringFeatures\"),\n",
        "            feature.StringIndexer(inputCol='Timezone', outputCol=\"TimezoneStringFeatures\"),\n",
        "            feature.StringIndexer(inputCol='Weather_Condition', outputCol=\"Weather_ConditionStringFeatures\", handleInvalid='keep'),\n",
        "            feature.StringIndexer(inputCol='Hour_Buckets_Named', outputCol=\"Hour_Buckets_Named_features\"),\n",
        "            feature.StringIndexer(inputCol='day_of_the_week', outputCol=\"day_of_the_week_features\")\n",
        "            ])\n",
        "stringIndexedCols = ['WindDirStringFeatures', 'TimezoneStringFeatures', \n",
        "                     'Weather_ConditionStringFeatures', 'Hour_Buckets_Named_features',\n",
        "                     'day_of_the_week_features']\n",
        "encoders1 = [\n",
        "            feature.OneHotEncoder(\n",
        "                inputCol=col,\n",
        "                outputCol=\"{0}_OHEFeatures\".format(col))\n",
        "            for col in stringIndexedCols\n",
        "            ]\n",
        "\n",
        "assembler1 = feature.VectorAssembler(\n",
        "    inputCols=[encoder.getOutputCol() for encoder in encoders1], \n",
        "    outputCol=\"OHEFeatures1\")\n",
        "\n",
        "encoders2 = [\n",
        "            feature.OneHotEncoder(\n",
        "                inputCol=col,\n",
        "                outputCol=\"{0}_OHEFeatures\".format(col))\n",
        "            for col in categoricalFeatures\n",
        "            ]\n",
        "\n",
        "assembler2 = feature.VectorAssembler(\n",
        "    inputCols=[encoder.getOutputCol() for encoder in encoders2], \n",
        "    outputCol=\"OHEFeatures2\")\n",
        "\n",
        "pca = feature.PCA(k=2, inputCol='OHEFeatures2', outputCol='pcaFeature')\n",
        "\n",
        "finalAssembler = feature.VectorAssembler(\n",
        "    inputCols=[\"featuresEngScaled\",\"OHEFeatures1\", \"pcaFeature\"],\n",
        "    outputCol=\"features\")\n",
        "\n",
        "reg = regression.DecisionTreeRegressor(featuresCol='features', labelCol='Length_Minutes')\n",
        "\n",
        "pipe_model = Pipeline(stages=[Indexer] + encoders1 + [assembler1] + encoders2 + [assembler2] + [pca] + [finalAssembler] + [reg])"
      ],
      "metadata": {
        "id": "CtuSf0bTAyKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model7 = pipe_model.fit(training_df)\n",
        "print(\"RMSE model VII: \", regression_evaluator.evaluate(model7.transform(validation_df)))\n",
        "print(\"R2 model VII: \", regression_evaluator.evaluate(model7.transform(validation_df), {regression_evaluator.metricName: \"r2\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LrcVfWhUz_Y",
        "outputId": "586b06fa-09b1-4642-d21c-e8e3b7cef377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE model VII:  58.00798837752092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction and Evaluation of Final Model:"
      ],
      "metadata": {
        "id": "gAYHO-WdgRZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using our best model(Model6) we make predictions for the test set. We are also using R2 here as a metric, R2 denotes the percentage varia"
      ],
      "metadata": {
        "id": "p2ubmu6mFWy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test RMSE model VI: \", regression_evaluator.evaluate(model6.transform(testing_df)))\n",
        "print(\"Test R2 model VI: \", regression_evaluator.evaluate(model6.transform(testing_df), {regression_evaluator.metricName: \"r2\"}))"
      ],
      "metadata": {
        "id": "DQBN0vX4gZzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fdc566-2feb-466a-93f9-20daefccbc0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test RMSE model VI:  54.19023847742889\n",
            "Test R2 model VI:  0.38865119488698596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Findings and Lessons Learned\n",
        "\n",
        "From the onset of the project, the team recognized the difficulty of this problem. The size of the dataset and variety of the observations make it extremely difficult to make accurate predictions. As noted before, the location of the accident was the most difficult to deal with. Every region, city and state has different kinds of roads that vary in traffic flow. A quantifier of how much traffic a road normally deals with would be a helpful datapoint in determining how it will be effected by an accident.\n",
        "Moreover, the dataset was very noisy as none of the columns in the dataset seem to be correlated strongly with the time of accident(our dependent feature).\n",
        "<br> <br>\n",
        "Despite this roadblock, the team set out to build the best regression model possible for predicting how long an accident would affect traffic given the information at our disposal. Random forest regressions fared better than linear regression models. This likely has to do with the sheer number of features. Many of the predictors were categorical and the others may not have had a linear relationship with Length_Minutes. As such, it isn't suprising that RF regression ended up being a better solution. \n",
        "<br><br>\n",
        "The best model we arrived at was a Random Forest regressor model. This model explains about 38.8% of the variation in the time of accident(in minutes) using the test set. It makes about 54 mins average prediction error on the test set.\n",
        "\n",
        "Though the models improved over the course of several iterations, there is still plenty of room for growth. We were confident that features derived from the description column would greatly boost our efforts. Though the LDA did improve the model, including features in the data frame seemed to greatly slow training. \n",
        "<br><br>\n",
        "There are several other questions that can still be answered using this dataset. A similar regression could be done with length of road affected as a target variable. A categorical classifier for the severity of the traffic could also be completed. This could be used to identify the time and location of accidents with great potential effect of traffic flow. Our team has just scratched the surface of accident traffic analysis and we would like to revisit this problem in the future. "
      ],
      "metadata": {
        "id": "be2ZSzFHWKhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QHSHNXliKkLD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "IST718_Group3_FinalCode.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "m_suF5XiJKkp",
        "O7BCwBbauhfI",
        "j7nFGYPtu-77",
        "hTPD--lZATAc",
        "jo7os8GcyUh1",
        "lQOCdCqSkWRt",
        "QF-5X7bHkYiz",
        "2eZiOo8jnNO2",
        "UMk6DG_T2QmM",
        "0LCNQeI99_2U",
        "NsL3-9nH8HfE",
        "rSDzxs948GMv",
        "Xv_6SVP0mhww"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}